{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21514169-4f4f-41af-b66d-cb5423333f14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937713f-f6d0-433d-8e02-0fb90957564e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tutorial 4\n",
    "\n",
    "UBC 2024-25\n",
    "\n",
    "## Outline\n",
    "\n",
    "During this tutorial, you will try hyperparameter optimization with `GridSearchCV` and `RandomizedSearchCV`, to find the best hyperparameters to perform a classification task using SVM RBF.\n",
    "\n",
    "All questions can be discussed with your classmates and the TAs - this is not a graded exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bf1f1-d4db-4ce0-9ed0-3683e20fac71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655c713-1f3e-414a-9d74-21e33bbaf36a",
   "metadata": {},
   "source": [
    "The goal of this classification problem is to predict which songs the user likes (target = 1) and which one they will not like (target = 0) based on some songs characteristics, like tempo and danceability.\n",
    "\n",
    "You have seen this dataset before (HW2), but take a moment to review its columns. You can also review the [source](https://www.kaggle.com/datasets/geomack/spotifyclassification). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03867573-d8e8-4c2a-942c-68e152dae80e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(DATA_DIR + \"spotify.csv\", index_col=0)\n",
    "spotify_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208e560-d1ee-4ca3-86b2-8640ccc01f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separating features and target \n",
    "# We are also removing the artist column for this exercise\n",
    "\n",
    "X_spotify = spotify_df.drop(columns=[\"target\", \"artist\"])\n",
    "y_spotify = spotify_df[\"target\"]\n",
    "\n",
    "# Creating training and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_spotify, y_spotify, test_size=0.2, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255f442-2452-4a34-9fbf-cfd85d7f6505",
   "metadata": {},
   "source": [
    "This dataset includes various types of features, so we will need to preprocess them before we are able to use them. The `ColumnTransformer` to do this is provided to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1bf2d-54ff-4810-a574-307d369d0c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_feats = ['acousticness', 'danceability', 'energy',\n",
    "                 'instrumentalness', 'liveness', 'loudness',\n",
    "                 'speechiness', 'tempo', 'valence'] # Scaling\n",
    "categorical_feats = ['time_signature', 'key'] # OHE\n",
    "passthrough_feats = ['mode']\n",
    "text_feat = \"song_title\" # Text processing (Bag of Words with CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31005c-7c24-487c-9ce0-9b997b506f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_feats), \n",
    "    (OneHotEncoder(handle_unknown = \"ignore\"), categorical_feats), \n",
    "    (\"passthrough\", passthrough_feats), \n",
    "    (CountVectorizer(max_features=50, stop_words=\"english\"), text_feat)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434f650-f2cd-4208-b805-ff9b2cfb191b",
   "metadata": {},
   "source": [
    "For this tutorial, we will focus on tuning a SVM RBF model. Other models may perform better, so typically we would not want to limit us to a single model without trying them out, but this will be enough to practice hyperparameter tuning.\n",
    "\n",
    "Run the next line to put the preprocessor and the `SVC` model in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31467626-c4ab-4b06-b138-46364ee6acba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_svm = make_pipeline(preprocessor, SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27510603-cdb2-4b2b-9040-0ad0387856ba",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 1</font>\n",
    "\n",
    "To have a base line to compare the improvements caused by hyperparameter tuning, let's first try the \"off the shelf\" model, the one with default parameters, and see what scores we get. \n",
    "\n",
    "By the way, the default values of C and gamma are `C = 1` and `gamma='scale'`, as visible on the [SVC documentation](https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html). `gamma='scale'` corresponds to a value of 1 / (n_features * X.var()), a robust default value (you can see Mike Gelbart, one of the course designers, arguing for it in this conversation in a [scikit-learn Github](https://github.com/scikit-learn/scikit-learn/issues/12741) issue).\n",
    "\n",
    "Use cross validation to see what the performance of our model is before we try to improve it. Reflect on the results: does it look like the model is good, or do we see signs of overfitting/underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5d2d5-b997-43cf-a958-8fb10d60031c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdedf8a1-092b-482d-8702-790e1c549a2c",
   "metadata": {},
   "source": [
    "Now that we have a baseline to compare future chages against, let's try other values of C and gamma. We will start from a small range, 1 to 6 for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e1e81-5c6b-426a-a9f5-85c0628ff5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svc__gamma\": [1, 2, 3, 4, 5, 6],\n",
    "    \"svc__C\": [1, 2, 3, 4, 5, 6],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151aa9b-dbac-4ebf-a277-a988dff31bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a grid search object \n",
    "gs = GridSearchCV(pipe_svm, \n",
    "                  param_grid = param_grid, \n",
    "                  n_jobs=-1, \n",
    "                  return_train_score=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3b753-7d1f-431b-81c0-457e6978a518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carry out the search \n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e9274-498e-47c6-b3df-07bf6689f6fd",
   "metadata": {},
   "source": [
    "As you have seen, once we have a GridSearchCV object, we can use it a bit like a model and call `fit` on it. In return:\n",
    "- It will search for the best hyperparameter values (within the grid)\n",
    "- You can access the best score and the best hyperparameters using `best_score_` and `best_params_` attributes, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe78fd-c9cc-4f13-9259-84221959f337",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 2</font>\n",
    "\n",
    "- How many combinations of parameters were tried in this grid search?\n",
    "- What is the best combination found?\n",
    "- What is the score achieved by this combination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08671b-916d-440d-92f6-8350fe188df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377c75e-74f0-472e-acc6-53d0b80f9351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb6655d-deb0-4306-b1d8-3941ddc88208",
   "metadata": {},
   "source": [
    "It seems that this search did not improve the result we found with default SVC, actually, it made it worse. Let's see what went wrong.\n",
    "\n",
    "We are going to use the function below to visualize the scores found in the grid search as a heat map (the function also repeats the grid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e748ea0-391e-430e-ac53-f593ef4bf206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_heatmap(param_grid, pipe, X_train, y_train):\n",
    "    grid_search = GridSearchCV(\n",
    "        pipe, param_grid, cv=5, n_jobs=-1, return_train_score=True\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    scores = np.array(results.mean_test_score).reshape(6, 6)\n",
    "\n",
    "    # plot the mean cross-validation scores\n",
    "    my_heatmap(\n",
    "        scores,\n",
    "        xlabel=\"gamma\",\n",
    "        xticklabels=param_grid[\"svc__gamma\"],\n",
    "        ylabel=\"C\",\n",
    "        yticklabels=param_grid[\"svc__C\"],\n",
    "        cmap=\"viridis\",\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28ea00-fa6a-4bd9-adbf-56fe47e7810c",
   "metadata": {},
   "source": [
    "Call the function to visualize the results of the grid search above as a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017a153-a632-4659-afab-4eac62081655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c32d8a15-31eb-46f0-9578-8f3dd274f28d",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 3</font>\n",
    "\n",
    "Based on this heat map, do you think we should try higher or lower values of C, to improve our results? What about gamma?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623da852-74ec-42c4-b641-ab387f963790",
   "metadata": {},
   "source": [
    "It turns out that our original choice of range for C and gamma was not a good one. The changes are too small to tell us anything interesting, and we are likely to have missed the best values combination.\n",
    "\n",
    "To improve our search, we will test a different set of values, much more different from each other. This will allow us to get more different results and hopefully get closer to the right combination, while limiting the number of attempts (remember that trying each combination takes time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645bb12-0362-4e0a-b9e9-52b6ccacbb0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    \"svc__gamma\": 10.0**np.arange(-4, 2, 1), \n",
    "    \"svc__C\": 10.0**np.arange(-2, 4, 1)\n",
    "}\n",
    "display_heatmap(param_grid1, pipe_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ac2b3-d21d-4498-9b9b-098278255ffd",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 4</font>\n",
    "\n",
    "Do you think that this heat map includes in its range the right combination of C and gamma? What are the best values found so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf3cd6-12b5-414b-bd65-413b3cb73f3c",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 5</font>\n",
    "\n",
    "As next step, you may try to \"zoom in\" closer to the good values found in the previous step, to see if they can be improved further. We know that the best hyperparameters are in this area, and it would be pointless to explore other regions of the map.\n",
    "\n",
    "Create a new parameter grid to explore new values in the region of interest. Please note that the grid needs to be 6x6 to work with the heat map function. You may enter your values manually, or try some useful functions to generate them, like `linspace()` and `logspace()` (examples below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3c86-5af9-4ec4-a04a-28abfefce4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.logspace(1, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461782f-1d03-4be2-823e-43469c0f84b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linspace(1, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527758c-028a-4a2e-9bd0-0346f32c7598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid2 = # Your grid here\n",
    "\n",
    "# display_heatmap(param_grid2, pipe_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd84c7-c0d3-4aa4-b558-6da474f5204f",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 6</font>\n",
    "\n",
    "The function `display_heatmap()` is useful for visualization, but unfortunately does not save the resulting parameters and model. For that, you will need to run `GridSearchCV`, or, if you know exactly what parameters you want for your model, create a new pipeline with the new parameters. \n",
    "\n",
    "Do either of these steps, then compare the validation score to the one obtained with defalut values. Were we able to improve the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337142d-e024-4d05-b145-b5f5897a0feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a9798c2-3063-4233-8296-78ee22d13a71",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 7</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0feaf-f066-4392-a177-edcee11a9c19",
   "metadata": {},
   "source": [
    "Without counting our first wrong attempt at grid search (the one with the small range), we tried 72 combinations before settling on the best hyperparameters we could find. As the number of possible hyperparameters increases, grid search can start taking a lot of time!\n",
    "\n",
    "Alternatively, we can use randomized search to test random combination of values within an interval. This is not as exhaustive as grid search, but it may still yield good results in a smaller amount of time. The cell below shows how to use `RandomizedSearchCV` to tune hyperparameters. \n",
    "\n",
    "We also added a new parameter: the `max_features` used by `CountVectorizer`. But this will not change the number of combinations tried (50, as per the parameter `n_iter`).\n",
    "\n",
    "- What are the best hyperparameters according to `RandomizedSearchCV`? How does this model compare to the one found using grid search?\n",
    "- Was it useful to try different values for `max_features`?\n",
    "- (Optional, but interesting) why are we using `loguniform` to specify the range for C and gamma, instead of specific values or a uniform distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bb1f0-24f1-43d3-8c73-517471901db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    \"columntransformer__countvectorizer__max_features\": [50, 100, 200, 400, 800, 1000, 2000],\n",
    "    \"svc__C\": loguniform(1e-3, 1e3),\n",
    "    \"svc__gamma\": loguniform(1e-5, 1e3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d14a5c-bf13-4c76-9c0e-6aaa039dc391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create a random search object\n",
    "random_search = RandomizedSearchCV(pipe_svm,                                    \n",
    "                  param_distributions = param_dist, \n",
    "                  n_iter=50, \n",
    "                  n_jobs=-1, \n",
    "                  return_train_score=True)\n",
    "\n",
    "# Carry out the search\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5267c-bcd6-4ecf-a795-fb1352f5c0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c2c68-6dd2-49bd-a891-e34f8d6de343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518ec29-897b-4146-906a-ec12380a7c7c",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 8</font>\n",
    "\n",
    "As always, our last step should be retraining the best model on the whole training set, and score it on the test set. Luckily for us, `GridSearchCV` and `RandomizedSearchCV` also fit a new model on the whole training set with the parameters that yielded the best results, so that is already taken care of!\n",
    "\n",
    "Pick you preferred model and score it on the test set. How well does the test score hold up compared to the validation score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc56a9-f721-4493-8c0d-55d2e6749bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "cpsc330"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
