{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d5691b-ddff-433b-bb28-e7e72d04a7ba",
   "metadata": {},
   "source": [
    "# Lecture 4: Class demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b1814-ff8b-4ac3-9b15-238c72a351fc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb038c-8631-4a32-a83b-c7434231016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), (\"..\"), \"code\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import panel as pn\n",
    "from panel import widgets\n",
    "from panel.interact import interact\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import mglearn\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from plotting_functions import *\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from utils import *\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"..\"), (\"..\"), \"data/\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b8a36-b935-4a87-8b95-8a769aeb303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670bfe1-d6d2-49e9-b9d5-a02692abdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "set_seed(seed=42)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c48ec-1874-4d7f-9c1d-dd5da5683c2d",
   "metadata": {},
   "source": [
    "To run this demo locally, you'll need to install `panel` in the cpsc330 environment. \n",
    "\n",
    "```\n",
    "conda install panel watchfiles\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62334a9e-62f1-45e2-b7d3-b408db8010d6",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c4265-c153-4064-bdaf-d0305bc70658",
   "metadata": {},
   "source": [
    "## Decision boundaries playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab5425-5a35-4ea9-a8a9-c027022b98da",
   "metadata": {},
   "source": [
    "In this interactive playground, you will investigate how various algorithms create decision boundaries to distinguish between Iris flower species using their sepal length and width as features. By adjusting the parameters, you can observe how the decision boundaries change, which can result in either overfitting (where the model fits the training data too closely) or underfitting (where the model is too simplistic).\n",
    "\n",
    "- With **k-Nearest Neighbours ($k$-NN)**, you'll determine how many neighboring flowers to consult. Should we rely on a single nearest neighbor? Or should we consider a wider group? \n",
    "\n",
    "- With **Support Vector Machine (SVM)** using the RBF kernel, you'll tweak the hyperparameters `C` and `gamma` to explore the tightrope walk between overly complex boundaries (that might overfit) and overly broad ones (that might underfit).\n",
    "  \n",
    "- With **Decision trees**, you'll observe the effect of `max_depth` on the decision boundary. \n",
    "\n",
    "Observe the process of refining decision boundaries, one parameter at a time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bb1f9-8781-42dd-921a-e29852f4e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and preprocessing\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.data\n",
    "iris_df['species'] = iris.target\n",
    "iris_df = iris_df[iris_df['species'] > 0]\n",
    "X, y = iris_df[['sepal length (cm)', 'sepal width (cm)']], iris_df['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)\n",
    "\n",
    "mglearn.discrete_scatter(\n",
    "    X_train[\"sepal length (cm)\"], X_train[\"sepal width (cm)\"], y_train, s=10\n",
    ");\n",
    "# Setting the labels for the x-axis and y-axis\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f178d3-ef59-4d59-a7f3-089279033d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da5196-e569-44b0-b4f0-1dcfe86fc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common plot settings\n",
    "def plot_results(model, X_train, y_train, title, ax):\n",
    "    mglearn.plots.plot_2d_separator(model, X_train.values, fill=True, alpha=0.4, ax=ax);\n",
    "    mglearn.discrete_scatter(\n",
    "        X_train[\"sepal length (cm)\"], X_train[\"sepal width (cm)\"], y_train, s=6, ax=ax\n",
    "    );\n",
    "    ax.set_xlabel(\"sepal length (cm)\", fontsize=10);\n",
    "    ax.set_ylabel(\"sepal width (cm)\", fontsize=10);\n",
    "    train_score = np.round(model.score(X_train.values, y_train), 2)\n",
    "    test_score = np.round(model.score(X_test.values, y_test), 2)\n",
    "    ax.set_title(\n",
    "        f\"{title}\\n train score = {train_score}\\ntest score = {test_score}\", fontsize=8\n",
    "    );\n",
    "    pass\n",
    "\n",
    "\n",
    "# Widgets for SVM, k-NN, and Decision Tree\n",
    "c_widget = pn.widgets.FloatSlider(\n",
    "    value=1.0, start=1, end=5, step=0.1, name=\"C (log scale)\"\n",
    ")\n",
    "gamma_widget = pn.widgets.FloatSlider(\n",
    "    value=1.0, start=-3, end=5, step=0.1, name=\"Gamma (log scale)\"\n",
    ")\n",
    "n_neighbors_widget = pn.widgets.IntSlider(\n",
    "    start=1, end=40, step=1, value=5, name=\"n_neighbors\"\n",
    ")\n",
    "max_depth_widget = pn.widgets.IntSlider(\n",
    "    start=1, end=20, step=1, value=3, name=\"max_depth\"\n",
    ")\n",
    "\n",
    "\n",
    "# The update function to create the plots\n",
    "def update_plots(c, gamma=1.0, n_neighbors=5, max_depth=3):\n",
    "    c_log = round(10**c, 2)  # Transform C to logarithmic scale\n",
    "    gamma_log = round(10**gamma, 2)  # Transform Gamma to logarithmic scale\n",
    "\n",
    "    fig = Figure(figsize=(9, 3))\n",
    "    axes = fig.subplots(1, 3)\n",
    "\n",
    "    models = [\n",
    "        SVC(C=c_log, gamma=gamma_log, random_state=42),\n",
    "        KNeighborsClassifier(n_neighbors=n_neighbors),\n",
    "        DecisionTreeClassifier(max_depth=max_depth, random_state=42),\n",
    "    ]\n",
    "    titles = [\n",
    "        f\"SVM (C={c_log}, gamma={gamma_log})\",\n",
    "        f\"k-NN (n_neighbors={n_neighbors})\",\n",
    "        f\"Decision Tree (max_depth={max_depth})\",\n",
    "    ]\n",
    "    for model, title, ax in zip(models, titles, axes):\n",
    "        model.fit(X_train, y_train)\n",
    "        plot_results(model, X_train, y_train, title, ax);\n",
    "    # print(c, gamma, n_neighbors, max_depth)\n",
    "    return pn.pane.Matplotlib(fig, tight=True);\n",
    "\n",
    "\n",
    "# Bind the function to the panel widgets\n",
    "interactive_plot = pn.bind(\n",
    "    update_plots,\n",
    "    c=c_widget.param.value_throttled,\n",
    "    gamma=gamma_widget.param.value_throttled,\n",
    "    n_neighbors=n_neighbors_widget.param.value_throttled,\n",
    "    max_depth=max_depth_widget.param.value_throttled,\n",
    ")\n",
    "\n",
    "# Layout the widgets and the plot\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(c_widget, n_neighbors_widget),\n",
    "    pn.Row(gamma_widget, max_depth_widget),\n",
    "    interactive_plot,\n",
    ")\n",
    "\n",
    "# Display the interactive dashboard\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee761522-b3b6-4a58-9611-6547832e5fb9",
   "metadata": {},
   "source": [
    "For this demonstration I'm using a subset of [Kaggle's Animal Faces dataset](https://www.kaggle.com/datasets/andrewmvd/animal-faces). I've put this subset in our [course GitHub repository](https://github.com/UBC-CS/cpsc330-2024W2/tree/main/lectures/data/animal_faces). \n",
    "\n",
    "The code in this notebook is a bit complicated and you are not expected to understand all the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee63d9c-3eaf-424b-9413-0b17822ef231",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0cb9a6-61ab-47b8-a4c3-eb4889d2ac7f",
   "metadata": {},
   "source": [
    "## Image classification using KNNs and SVM RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edecc32-24e4-4350-9bd9-fa926193a799",
   "metadata": {},
   "source": [
    "Let's proceed with reading the data. Since we don't have tabular data, we are using a slightly more complex method to read it. You don't need to understand the code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1de9d-742b-4cd3-b57d-e23c6f593c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "IMAGE_SIZE = 200\n",
    "def read_img_dataset(data_dir):     \n",
    "    \"\"\"\n",
    "    Reads and preprocesses an image dataset from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The directory path where the dataset is located.\n",
    "\n",
    "    Returns:\n",
    "        inputs (Tensor): A batch of preprocessed input images.\n",
    "        classes (Tensor): The corresponding class labels for the input images.\n",
    "    \"\"\"    \n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),     \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),            \n",
    "        ])\n",
    "               \n",
    "    image_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "         image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0 \n",
    "    )\n",
    "    dataset_size = len(image_dataset)\n",
    "    class_names = image_dataset.classes\n",
    "    inputs, classes = next(iter(dataloader))\n",
    "    return inputs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25ca9d-4d58-4952-bc18-af2a24e089f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_imgs(inputs):\n",
    "    plt.figure(figsize=(10, 70)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\n",
    "    plt.imshow(np.transpose(utils.make_grid(inputs, padding=1, normalize=True),(1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57336b31-dfa6-4908-8dc0-51f62e9da92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = DATA_DIR + \"animal_faces/train\" # training data\n",
    "file_names = [image_file for image_file in glob.glob(train_dir + \"/*/*.jpg\")]\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "X_anim_train, y_train = read_img_dataset(train_dir)\n",
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd3d65-efd8-45a7-9984-23553dce5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir = DATA_DIR + \"animal_faces/valid\" # valid data\n",
    "file_names = [image_file for image_file in glob.glob(valid_dir + \"/*/*.jpg\")]\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "X_anim_valid, y_valid = read_img_dataset(valid_dir)\n",
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d349a1-400f-4405-953a-5bb15cd2c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_anim_train.numpy()\n",
    "X_valid = X_anim_valid.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e7f9e-a99f-4c79-a178-1e8fb99b9a98",
   "metadata": {},
   "source": [
    "Let's examine some of the sample images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b14f2-fe64-428a-9b56-ce7a8d98afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_imgs(X_anim_train[0:24,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c674f8-f496-48b6-acd6-13292653b523",
   "metadata": {},
   "source": [
    "With K-nearest neighbours (KNN), we will attempt to classify an animal face into one of three categories: cat, dog, or wild animal. The idea is that when presented with a new animal face image, we want the model to assign it to one of these three classes based on its similarity to other images within each of these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a9b52-498a-4742-812a-47bb3a795bdb",
   "metadata": {},
   "source": [
    "To train a KNN model, we require tabular data. How can we transform image data, which includes height and width information, into tabular data with meaningful numerical values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca6eaa-dc4b-40c3-abf7-6ef9f48a9225",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb988d-ca9a-4418-bffb-0822fed2ae52",
   "metadata": {},
   "source": [
    "Flattening images and feeding them to K-nearest neighbors (KNN) is one approach. However, in this demonstration, we will explore an alternative method. We will employ a pre-trained image classification model known as 'desenet' to obtain a 1024-dimensional meaningful representation of each image. The function provided below accomplishes this task for us. Once again, you are not required to comprehend the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ede40e-a951-4906-8576-8002a73c302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, inputs):\n",
    "    \"\"\"\n",
    "    Extracts features from a pre-trained DenseNet model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A pre-trained DenseNet model.\n",
    "        inputs (torch.Tensor): Input data for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted features from the model.\n",
    "    \"\"\"   \n",
    "    \n",
    "    with torch.no_grad():  # turn off computational graph stuff\n",
    "        Z_train = torch.empty((0, 1024))  # Initialize empty tensors\n",
    "        y_train = torch.empty((0))\n",
    "        Z_train = torch.cat((Z_train, model(inputs)), dim=0)\n",
    "    return Z_train.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaa57c-e733-4fe5-9026-544dc7ebbdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\n",
    "densenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef9b8b-cf06-4c07-91d0-f952877da01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0aec4-4120-4354-9055-95b9db384011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get representations of the train images\n",
    "Z_train = get_features(\n",
    "    densenet, X_anim_train, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f81287-0b93-4aa1-836b-8da3e6642a3d",
   "metadata": {},
   "source": [
    "We now have tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70856c-05c9-43e1-94f9-b32298b8f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366a0e2-c96d-4db2-b2d2-d941c3d46ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ba7b0-7719-4cbd-9dbf-a41b74133212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get representations of the validation images\n",
    "Z_valid = get_features(\n",
    "    densenet, X_anim_valid, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db204d02-4465-4d8e-9c9d-8af481ea9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea06648-6216-4581-b922-757d172bb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af9494-5a8e-4ced-935f-0cae87dfc886",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03a781-2244-4dfb-acff-8ef0759fad94",
   "metadata": {},
   "source": [
    "### Dummy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7834e6-57c0-440a-9991-1fc893c234f2",
   "metadata": {},
   "source": [
    "Let's examine the baseline accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73679442-d8ec-4847-8ae9-3ec5db47fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "pd.DataFrame(cross_validate(dummy, Z_train, y_train, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff9b83-110c-4f22-bbc7-15023852f0c4",
   "metadata": {},
   "source": [
    "## Classification with `KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10650180-5efb-4084-b41a-b6d9f89b684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "pd.DataFrame(cross_validate(knn, Z_train, y_train, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb029bd-c000-4b69-835a-869d44582c12",
   "metadata": {},
   "source": [
    "This is with the default `n_neighbors`. Let's optimize `n_neighbors`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f68223-e0fd-4e79-a3fb-973de6f22b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.get_params()['n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63288c5d-333f-420f-af7e-f2c13a20e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52c856-7fd8-4143-945a-778b62933898",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = np.arange(1, 15, 1).tolist()\n",
    "\n",
    "results_dict = {\n",
    "    \"n_neighbors\": [],\n",
    "    \"mean_train_score\": [],\n",
    "    \"mean_cv_score\": [],\n",
    "    \"std_cv_score\": [],\n",
    "    \"std_train_score\": [],\n",
    "}\n",
    "\n",
    "for k in n_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_validate(knn, Z_train, y_train, return_train_score=True)\n",
    "    results_dict[\"n_neighbors\"].append(k)\n",
    "\n",
    "    results_dict[\"mean_cv_score\"].append(np.mean(scores[\"test_score\"]))\n",
    "    results_dict[\"mean_train_score\"].append(np.mean(scores[\"train_score\"]))\n",
    "    results_dict[\"std_cv_score\"].append(scores[\"test_score\"].std())\n",
    "    results_dict[\"std_train_score\"].append(scores[\"train_score\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8409b1-3ae4-4ca6-8673-ae8057dc6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df = results_df.set_index(\"n_neighbors\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa842b-b14c-4e23-af27-fecc5b5a8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['mean_train_score', 'mean_cv_score']].plot(ylabel='Accuracy', title=\"k vs. accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7023bb-a145-470e-a9d3-26cea34a1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = n_neighbors[np.argmax(results_df['mean_cv_score'])]\n",
    "best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbc606-e4c9-4d2e-9cb1-225a60c7480f",
   "metadata": {},
   "source": [
    "Is SVC performing better than k-NN? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb2b3d-51ab-4998-8c9c-b5b029c0bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.logspace(-1, 2, 4)\n",
    "cv_scores = []\n",
    "train_scores = []\n",
    "\n",
    "for C_val in C_values:\n",
    "    print('C = ', C_val)\n",
    "    svc = SVC(C=C_val)\n",
    "    scores = cross_validate(svc, Z_train, y_train, return_train_score=True)\n",
    "    cv_scores.append(scores['test_score'].mean())\n",
    "    train_scores.append(scores['train_score'].mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bae52-6524-446a-9a00-92fefd812085",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\"cv\": cv_scores, \n",
    "                           \"train\": train_scores},index = C_values)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b53775-3daf-4805-b5b9-5b669985a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = C_values[np.argmax(results_df['cv'])]\n",
    "best_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1646be2-389f-42ac-a073-a29937a81d9e",
   "metadata": {},
   "source": [
    "Let's go back to KNN and manually examine the nearest neighbours. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf7027-2906-4afe-ae77-abe2f0a0c0f7",
   "metadata": {},
   "source": [
    "What are the nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e25ea5-e357-45d6-b0e2-5efa606dfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors()\n",
    "nn.fit(Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc572f3-58f7-48e2-9b80-04804e931471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not have to understand this code. \n",
    "def show_nearest_neighbors(test_idx, nn, Z, X, y):\n",
    "    distances, neighs = nn.kneighbors([Z[test_idx]])\n",
    "    neighbors = neighs.ravel()\n",
    "    plt.figure(figsize=(2,2), dpi=80)\n",
    "    query_img = X[test_idx].transpose(1, 2, 0)\n",
    "    query_img = ((X[test_idx].transpose(1, 2, 0) + 1.0) * 0.5 * 255).astype(np.uint8)\n",
    "    plt.title('Query image', size=12)\n",
    "    plt.imshow(np.clip(query_img, 0, 255));\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())    \n",
    "    plt.show()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10,4), subplot_kw={'xticks':(), 'yticks':()})\n",
    "    print('Nearest neighbours:')\n",
    "    \n",
    "    for ax, dist, img_ind in zip(axes.ravel(), distances.ravel(), neighbors):\n",
    "        img = X_train[img_ind].transpose(1, 2, 0)\n",
    "        img = ((X_train[img_ind].transpose(1, 2, 0) + 1.0) * 0.5 * 255).astype(np.uint8)\n",
    "        ax.imshow(np.clip(img, 0, 255))\n",
    "        ax.set_title('distance: '+ str(round(dist,3)), size=10 )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44555ac-26a2-4b84-b341-910e6f31c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [1, 2, 32, 70, 4]\n",
    "for idx in test_idx: \n",
    "    show_nearest_neighbors(idx, nn, Z_valid, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961525a-1dfe-4f55-a212-18d7bf4460bb",
   "metadata": {},
   "source": [
    "We can use $k$-NNs for more than just classifying images; we can also find the most similar examples in a dataset. You can see how this would be useful in recommendation systems. For instance, if a user is purchasing an item, you can find similar items in the dataset and recommend them to the user! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
